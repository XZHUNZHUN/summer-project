{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"drum.ipynb","version":"0.3.2","provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fWTZvClry1ng"},"source":["# **drum**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"r3RWCmH0y1nq"},"source":["## **load data**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ob72uX8gy1ns","colab":{}},"source":["import torch\n","from torch import autograd\n","from torch.utils.data import DataLoader\n","from torch import optim\n","import torch.nn as nn\n","import torch.nn.parallel\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import librosa\n","import librosa.display\n","import pickle as pk\n","class AudioDatasets(torch.utils.data.Dataset):\n","    def __init__(self):\n","      self.wave_num = 16384    \n","    def __getitem__(self, index):\n","        \n","        song = 1\n","        family = 1\n","        index %= 320 #352 file divide 176 176\n","        index += 1166\n","        loadpath = './drive/My Drive/drums/npy/{}.npy'.format(index)\n","        wave = np.load(loadpath)\n","        wave = wave.reshape(1,self.wave_num)\n","        wave = torch.from_numpy(wave)\n","\n","        return wave, song, family\n","    def __len__(self):\n","        return 320\n","        \n","def loadData(batch_size):\n","   \n","    trainsets = AudioDatasets()\n","    trainloader = DataLoader(trainsets,batch_size=batch_size, shuffle=True, num_workers=10)\n","    return trainloader"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"luDNxNg_y1nv"},"source":["## **phase shuffle**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"U0ApgEvpy1nw","colab":{}},"source":["#copy from https://github.com/chrisdonahue/wavegan\n","class PhaseShuffle(nn.Module):\n","    \"\"\"\n","    Performs phase shuffling, i.e. shifting feature axis of a 3D tensor\n","    by a random integer in {-n, n} and performing reflection padding where\n","    necessary\n","    If batch shuffle is enabled, only a single shuffle is applied to the entire\n","    batch, rather than each sample in the batch.\n","    \"\"\"\n","\n","    def __init__(self, shift_factor, batch_shuffle=False):\n","        super(PhaseShuffle, self).__init__()\n","        self.shift_factor = shift_factor\n","        self.batch_shuffle = batch_shuffle\n","\n","    def forward(self, x):\n","        # Return x if phase shift is disabled\n","        if self.shift_factor == 0:\n","            return x\n","\n","        if self.batch_shuffle:\n","            # Make sure to use PyTorcTrueh to generate number RNG state is all shared\n","            k = int(torch.Tensor(1).random_(0, 2*self.shift_factor + 1)) - self.shift_factor\n","\n","            # Return if no phase shift\n","            if k == 0:\n","                return x\n","\n","            # Slice feature dimension\n","            if k > 0:\n","                x_trunc = x[:, :, :-k]\n","                pad = (k, 0)\n","            else:\n","                x_trunc = x[:, :, -k:]\n","                pad = (0, -k)\n","\n","            # Reflection padding\n","            x_shuffle = F.pad(x_trunc, pad, mode='reflect')\n","\n","        else:\n","            # Generate shifts for each sample in the batch\n","            k_list = torch.Tensor(x.shape[0]).random_(0, 2*self.shift_factor+1)\\\n","                - self.shift_factor\n","            k_list = k_list.numpy().astype(int)\n","\n","            # Combine sample indices into lists so that less shuffle operations\n","            # need to be performed\n","            k_map = {}\n","            for idx, k in enumerate(k_list):\n","                k = int(k)\n","                if k not in k_map:\n","                    k_map[k] = []\n","                k_map[k].append(idx)\n","\n","            # Make a copy of x for our output\n","            x_shuffle = x.clone()\n","\n","            # Apply shuffle to each sample\n","            for k, idxs in k_map.items():\n","                if k > 0:\n","                    x_shuffle[idxs] = F.pad(x[idxs][..., :-k], (k,0), mode='reflect')\n","                else:\n","                    x_shuffle[idxs] = F.pad(x[idxs][..., -k:], (0,-k), mode='reflect')\n","\n","        assert x_shuffle.shape == x.shape, \"{}, {}\".format(x_shuffle.shape,\n","                                                           x.shape)\n","        return x_shuffle"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"R0Ee0qfpy1nz"},"source":["## **dicriminator**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"k8Uh5LMqy1nz","colab":{}},"source":["class Discriminator(nn.Module):\n","    def __init__(self, model_size=64, num_channels=1, shift_factor=2, alpha=0.2, batch_shuffle=False):\n","        super(Discriminator, self).__init__()\n","        self.model_size = model_size # d\n","        self.num_channels = num_channels # c\n","        self.shift_factor = shift_factor # n\n","        self.alpha = alpha\n","\n","        # Conv2d(in_channels, out_channels, kernel_size, stride=1, etc.)\n","        self.conv1 = nn.DataParallel(nn.Conv1d(num_channels, model_size, 25, stride=4, padding=11))\n","        self.conv2 = nn.DataParallel(\n","            nn.Conv1d(model_size, 2 * model_size, 25, stride=4, padding=11))\n","        self.conv3 = nn.DataParallel(\n","            nn.Conv1d(2 * model_size, 4 * model_size, 25, stride=4, padding=11))\n","        self.conv4 = nn.DataParallel(\n","            nn.Conv1d(4 * model_size, 8 * model_size, 25, stride=4, padding=11))\n","        self.conv5 = nn.DataParallel(\n","            nn.Conv1d(8 * model_size, 16 * model_size, 25, stride=4, padding=11))\n","        self.ps1 = PhaseShuffle(shift_factor, batch_shuffle=batch_shuffle)\n","        self.ps2 = PhaseShuffle(shift_factor, batch_shuffle=batch_shuffle)\n","        self.ps3 = PhaseShuffle(shift_factor, batch_shuffle=batch_shuffle)\n","        self.ps4 = PhaseShuffle(shift_factor, batch_shuffle=batch_shuffle)\n","        self.fc1 = nn.DataParallel(nn.Linear(256 * model_size, 1))\n","\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n","                nn.init.kaiming_normal_(m.weight.data)\n","\n","    def forward(self, x):\n","        x = F.leaky_relu(self.conv1(x), negative_slope=self.alpha)\n","        x = self.ps1(x)\n","\n","        x = F.leaky_relu(self.conv2(x), negative_slope=self.alpha)\n","        x = self.ps2(x)\n","\n","        x = F.leaky_relu(self.conv3(x), negative_slope=self.alpha)\n","        x = self.ps3(x)\n","\n","        x = F.leaky_relu(self.conv4(x), negative_slope=self.alpha)\n","        x = self.ps4(x)\n","\n","        x = F.leaky_relu(self.conv5(x), negative_slope=self.alpha)\n","\n","        x = x.view(-1, 256 * self.model_size)\n","\n","        return torch.sigmoid(self.fc1(x))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"42re1HmEy1n2"},"source":["## **generator**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rhd_Iw4ry1n3","colab":{}},"source":["class Generator(nn.Module):\n","    def __init__(self, model_size=64, num_channels=1, latent_dim=100,\n","                 post_proc_filt_len=512):\n","        super(Generator, self).__init__()\n","        self.model_size = model_size # d\n","        self.num_channels = num_channels # c\n","        self.latent_dim = latent_dim\n","        self.post_proc_filt_len = post_proc_filt_len\n","        \n","        self.fc1 = nn.DataParallel(nn.Linear(latent_dim, 256 * model_size))\n","        \n","        self.tconv1 = None\n","        self.tconv2 = None\n","        self.tconv3 = None\n","        self.tconv4 = None\n","        self.tconv5 = None\n","        \n","\n","        self.tconv1 = nn.DataParallel(\n","                 nn.ConvTranspose1d(16 * model_size, 8 * model_size, 25, stride=4, padding=11,\n","                                    output_padding=1))\n","        self.tconv2 = nn.DataParallel(\n","                 nn.ConvTranspose1d(8 * model_size, 4 * model_size, 25, stride=4, padding=11,\n","                                    output_padding=1))\n","        self.tconv3 = nn.DataParallel(\n","                 nn.ConvTranspose1d(4 * model_size, 2 * model_size, 25, stride=4, padding=11,\n","                                    output_padding=1))\n","        self.tconv4 = nn.DataParallel(\n","                 nn.ConvTranspose1d(2 * model_size, model_size, 25, stride=4, padding=11,\n","                                    output_padding=1))\n","        self.tconv5 = nn.DataParallel(\n","                 nn.ConvTranspose1d(model_size, num_channels, 25, stride=4, padding=11,\n","                                    output_padding=1))\n","\n","        \n","        if post_proc_filt_len:\n","            self.ppfilter1 = nn.DataParallel(nn.Conv1d(num_channels, num_channels, post_proc_filt_len))\n","        \n","        for m in self.modules():\n","            if isinstance(m, nn.ConvTranspose1d) or isinstance(m, nn.Linear):\n","                nn.init.kaiming_normal_(m.weight.data)\n","\n","    def forward(self, x):\n","\n","        x = self.fc1(x).view(-1, 16 * self.model_size, 16)\n","        x = F.relu(x)\n","        output = None\n","        \n","        x = F.relu(self.tconv1(x))\n","        x = F.relu(self.tconv2(x))\n","        x = F.relu(self.tconv3(x))\n","        x = F.relu(self.tconv4(x))\n","        output = torch.tanh(self.tconv5(x))\n","                    \n","        if self.post_proc_filt_len:\n","            # Pad for \"same\" filtering\n","            if (self.post_proc_filt_len % 2) == 0:\n","                pad_left = self.post_proc_filt_len // 2\n","                pad_right = pad_left - 1\n","            else:\n","                pad_left = (self.post_proc_filt_len - 1) // 2\n","                pad_right = pad_left\n","            output = self.ppfilter1(F.pad(output, (pad_left, pad_right)))\n","        \n","        return output"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"OmmGyZDmy1n5"},"source":["## **loss compute**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Rrm_u6AJy1n6","colab":{}},"source":["#compute D and G loss\n","def cal_D_loss(D,G,real_wave,noisev):\n","  D.zero_grad()\n","  #convenient values for\n","  one = torch.FloatTensor([1])\n","  mone = one * -1\n","  one = one.cuda()\n","  mone = mone.cuda()\n","  \n","  # train with real\n","  real_out = D(real_wave)\n","  real_out = real_out.mean()\n","  real_out.backward(mone)\n","\n","  # train fake wave\n","  #put the noise through the Generator\n","  fake_wave = autograd.Variable(G(noisev).data)\n","  #print(fake_wave.shape)\n","  inputv = fake_wave\n","  fake_out = D(inputv)\n","  fake_out = fake_out.mean()\n","  fake_out.backward(one)\n","\n","  #train with gradient penalty\n","  gradient_penalty = calc_gradient_penalty(D, real_wave.data, fake_wave.data, batch_size)\n","  gradient_penalty.backward(one)\n","\n","  D_lost = fake_out - real_out + gradient_penalty\n","  Wass_D = real_out - fake_out\n","  \n","  return D_lost, Wass_D\n","\n","def cal_G_loss(G,D,batch_size, laten_dim):\n","  #train Generator\n","  G.zero_grad()\n","  \n","  #convenient values for\n","  one = torch.FloatTensor([1])\n","  mone = one * -1\n","  one = one.cuda()\n","  mone = mone.cuda()\n","  \n","  #generate nosie\n","  noise = torch.Tensor(batch_size, laten_dim).uniform_(-1, 1)\n","  noisev = autograd.Variable(noise.cuda(), requires_grad=False)\n","  #through the generator\n","  fake_wave = G(noisev)\n","  output = D(fake_wave)\n","  output = output.mean()\n","  output.backward(mone)\n","  G_lost = - output\n","  return G_lost\n","\n","\n","def calc_gradient_penalty(netD, real_data, fake_data, batch_size):\n","  alpha = torch.rand(batch_size, 1, 1)\n","  alpha = alpha.expand(real_data.size())\n","  alpha = alpha.cuda()\n","\n","  interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n","  interpolates = interpolates.cuda()\n","  \n","  interpolates = autograd.Variable(interpolates, requires_grad=True)\n","\n","  disc_interpolates = netD(interpolates)\n","\n","  gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n","                            grad_outputs=torch.ones(disc_interpolates.size()).cuda(),\n","                            create_graph=True, retain_graph=True, only_inputs=True)[0]\n","  \n","  gradients = gradients.view(gradients.size(0), -1)\n","\n","  gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * 0.1\n","  return gradient_penalty"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0F19Fa9Xy1n8"},"source":["## **save model and save audio**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"fcHAyLJzy1n9","colab":{}},"source":["\n","def save_sample(data,sample_size,epoch):\n","  for i in range(sample_size):\n","    sample = data[i].reshape(16384,1)    \n","    \n","    librosa.output.write_wav('./drive/My Drive/drums/sample/{}_{}.wav'.format(epoch,i),sample, 16000)\n","\n","def showing_wave(data):\n","  sample = data.reshape(16384)\n","  print(sample)\n","  plt.figure(figsize=(25,8))\n","  librosa.display.waveplot(sample,16000)\n","  plt.show()\n","  plt.close()\n","\n","def plot_loss(G,D):\n","  plt.cla()\n","  plt.plot(G,c='#4AD631',label='g_loss')\n","  plt.plot(D,label='d_loss')\n","  plt.xlabel('epoch')\n","  plt.ylabel('Loss')\n","  plt.title('learning rate 1e-6 drum')\n","  plt.legend()\n","  plt.show()\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CmHru4S2y1n_"},"source":["## **Training**"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A8_RZ948y1oA","outputId":"1d7ca693-1df1-437b-e09e-691c80a102f4","executionInfo":{"status":"error","timestamp":1566217671969,"user_tz":-60,"elapsed":5902,"user":{"displayName":"zhun xu","photoUrl":"https://lh6.googleusercontent.com/--2MCZCR2zhY/AAAAAAAAAAI/AAAAAAAAAAc/mOtRWmri120/s64/photo.jpg","userId":"11884665079169266016"}},"colab":{"base_uri":"https://localhost:8080/","height":270}},"source":["import time\n","if __name__==\"__main__\":\n","    #hyperparameter\n","  \n","    batch_size = 64\n","    batch_per_epoch = 1\n","    epoch = 500\n","    d_epoch = 5\n","    lr = 0.000001 #0.0001, 0.00005,  0.00001,  0.000005,  0.000001,  0.0000005\n","    beta_1 = 0.5\n","    beta_2 = 0.9\n","    laten_dim = 100\n","    sample_size = 2\n","    sample_per_epoch = 1\n","    model_per_epoch = 1\n","    load_model = True\n","    D = Discriminator(batch_shuffle=True)\n","    G = Generator()\n","    \n","    #---------training times-----------\n","    TRAIN_TIME = '1_'\n","    main_path = './drive/My Drive/drums/'\n","  \n","    G_model_path = main_path + 'model/G/'+ TRAIN_TIME\n","    D_model_path = main_path + 'model/D/'+ TRAIN_TIME\n","        \n","    #load the parameter of network \n","    if not load_model:\n","      print('loading model...')\n","      \n","      G.load_state_dict(torch.load(main_path + 'model/G/drum_clap.pkl'))\n","      #D.load_state_dict(torch.load(main_path + 'model/D/2_1600.pkl'))  \n","    \n","    #save history\n","    history = []\n","    plot_history_D = []\n","    plot_history_G = []\n","    \n","    \n","    \n","    #optimizer\n","    d_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(beta_1,beta_2))\n","    g_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(beta_1,beta_2))\n","\n","    '''\n","    #========= test generator ==============#\n","    #sample noise\n","    sample_noise = torch.randn(sample_size, laten_dim)\n","    sample_noisev = autograd.Variable(sample_noise.cuda())\n","    sample_output = G(sample_noisev)\n","    sample_output = sample_output.cpu()\n","    for j in range(sample_size):\n","      showing_wave(sample_output.data.numpy()[j])\n","    save_sample(sample_output.data.numpy(),sample_size,'drumclap')\n","    \n","    '''\n","    start_time = time.clock()\n","    for epoch_iter in range(epoch):\n","        tolerrent = 0\n","        trainloader = loadData(batch_size)\n","        iter_train = iter(trainloader)\n","        history_epoch = {\n","            'D':{\n","                'loss':[],\n","                'wass_loss':[],\n","                'valid_loss':[],\n","                'valid_wass':[]\n","            },\n","            'G':{\n","                'loss':[]\n","            }\n","        }\n","        \n","        epoch_time = time.clock()\n","        print(\"Epoch: {}/{}\".format(epoch_iter + 1, epoch))\n","        #iteration the batch size\n","        for batch_iter in range(batch_per_epoch):\n","            print(\"Batch: {}/{} \".format(batch_iter + 1, batch_per_epoch),end='')\n"," \n","            \n","            #set model parameters to require gradients to be computer and stored\n","            for p in D.parameters():\n","                p.requires_grad = True\n","# =============================================================================\n","#           (1) training the D model\n","# =============================================================================\n","            for d_iter in range(d_epoch): #5 iters\n","                print('#',end='')\n","                #get next train data from dataloader (64,1,16384)\n","                try:\n","                  traindata = next(iter_train)\n","                except:\n","                  print('\\n ---------------- data loader exauted ---------------')\n","                  \n","                  trainloader = loadData(batch_size)\n","                  iter_train = iter(trainloader)\n","                  traindata = next(iter_train)\n","               \n","                real_wave = autograd.Variable(traindata[0].cuda())\n","                #generate nosie\n","                noise = torch.Tensor(batch_size, laten_dim).uniform_(-1, 1)\n","                \n","                noisev = autograd.Variable(noise.cuda(), requires_grad=False)\n","                \n","                #compute loss\n","                D_loss, Wass_D = cal_D_loss( D, G, real_wave, noisev)                                               \n","                d_optimizer.step()\n","                \n","                #D_valid_loss, Wass_valid_loss = cal_D_loss( D, G, real_wave, noisev)\n","                \n","                D_loss = D_loss.cpu()\n","                Wass_D = Wass_D.cpu()\n","                #D_valid_loss = D_valid_loss.cpu()\n","                #Wass_valid_loss = Wass_valid_loss.cpu()\n","                \n","                #save history\n","                #_temp = history_epoch['D']\n","                history_epoch['D']['loss'].append(D_loss.data.numpy())\n","                history_epoch['D']['wass_loss'].append(Wass_D.data.numpy())\n","                #history_epoch['D']['valid_loss'].append(D_valid_loss.data.numpy())\n","                #history_epoch['D']['valid_wass'].append(Wass_valid_loss.data.numpy())\n","                \n","\n","                \n","# =============================================================================\n","#             (2)updade G model\n","# =============================================================================\n","            print('#',end='')\n","            #fix the D parameters\n","            for p in D.parameters():\n","                p.requires_grad = False\n","                \n","            \n","            G_lost = cal_G_loss(G,D,batch_size, laten_dim)\n","            \n","            g_optimizer.step()\n","            G_lost = G_lost.cpu()\n","\n","\n","            history_epoch['G']['loss'].append(G_lost.data.numpy())\n","\n","            \n","\n","            #print accuracy\n","            print(' d_loss: {:.4f},'\n","                    'd_wass: {:.4f},'\n","                    'g_loss: {:.4f}'.format(\n","                    history_epoch['D']['loss'][-1], \n","                    history_epoch['D']['wass_loss'][-1],                   \n","                    history_epoch['G']['loss'][-1]))\n","            plot_history_D.append(history_epoch['D']['loss'][-1])\n","            plot_history_G.append(history_epoch['G']['loss'][-1])\n","            #accelerate lr\n","            if history_epoch['G']['loss'][-1] > -0.0001:\n","              tolerrent += 1\n","\n","            #print (print_temp)\n","# =============================================================================\n","#       save the sample    \n","# =============================================================================\n","        #change lr\n","        \n","        if tolerrent >= 7 and lr > 0.000001:\n","          lr = 0.000001\n","          d_optimizer.param_groups[0]['lr'] = lr\n","          g_optimizer.param_groups[0]['lr'] = lr\n","        \n","          print('------------------changing learning rate to {}!------------------'.format(lr))\n","        \n","        #print time\n","        time_a = int(time.clock()-start_time)/60\n","        time_b = int(time.clock()-epoch_time)/60\n","        print('epoch time: {:.2f} min || total time: {:.2f} min'.format(time_b, time_a))\n","        \n","        #append the history\n","        history.append(history_epoch)\n","        \n","        #save the model\n","        if (epoch_iter+1)%200 == 0:\n","          print('saving the model...')  \n","          try:\n","            torch.save(G.state_dict(), G_model_path + str(epoch_iter+1) + '.pkl', pickle_protocol=pk.HIGHEST_PROTOCOL)\n","            torch.save(D.state_dict(), D_model_path + str(epoch_iter+1) + '.pkl', pickle_protocol=pk.HIGHEST_PROTOCOL)\n","          except:\n","            print('failed to save model')\n","\n","        #sample noise\n","        sample_noise = torch.Tensor(sample_size, laten_dim).uniform_(-1, 1)\n","        sample_noisev = autograd.Variable(sample_noise.cuda(), requires_grad=False)\n","\n","\n","        #plot the loss\n","        if (epoch_iter+1)%10 == 0:\n","          plot_loss(plot_history_G,plot_history_D)\n","        #showing the wave plt\n","        if (epoch_iter+1)%50 == 0:\n","          sample_output = G(sample_noisev)\n","          sample_output = sample_output.cpu()\n","          showing_wave(sample_output.data.numpy()[0])\n","          \n","        if (epoch_iter+1)%200 == 0:\n","          print('saving the sample...') \n","          try:\n","            save_sample(sample_output.data.numpy(),sample_size,epoch_iter+1)\n","          except:\n","            print('failed to save sample')\n","        \n","        \n","          "],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch: 1/500\n","Batch: 1/1 ##"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-045af0133c51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m                 \u001b[0;31m#D_valid_loss, Wass_valid_loss = cal_D_loss( D, G, real_wave, noisev)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0mD_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mD_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0mWass_D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWass_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0;31m#D_valid_loss = D_valid_loss.cpu()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]}]}